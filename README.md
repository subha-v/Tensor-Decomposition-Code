# Tensor-Decomposition-Code
Neural networks have revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to their unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications.  In this work, we study the effect of tensor decomposition methods on multimodal vision-language neural networks. Specifically, we explore the efficacy of TensorTrain decomposition in compressing Transformer-based models. Our investigation utilizes BERT as the primary text encoder, while experimenting with ViT and EfficientNet as vision encoders. To evaluate our approach, we employed the CIFAR10 image classification dataset. Our approach is unique as it significantly improves the accuracy of existing models by up to 10%, all without the need for post-training adjustments. Additionally, we demonstrate the robustness of our method when considering various tensor shapes and sizes. To broaden our impact, we perform a layer-by-layer analysis on the decomposed weights, isolating factors that can potentially impact model accuracy.
